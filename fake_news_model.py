# -*- coding: utf-8 -*-
"""Fake_News_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nxyrDd0xx25vSNE15xHptcNpC1wITdtB
"""

from google.colab import drive
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import classification_report, confusion_matrix

import pickle

"""# Data Preprocessing"""

drive.mount('/content/drive')

news = "/content/drive/MyDrive/Colab Notebooks/News Data/News.csv"
df = pd.read_csv(news)
low_memory=False

"""Shuffle so the dataset isn't all Fake and then all Real articles"""

from sklearn.utils import shuffle
df = shuffle(df)
df = df.reset_index(drop=True)

"""Remove the unnamed column to clean up the dataframe"""

df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

"""Check for any null values that would upset the processing"""

print (df.isnull().sum())
df.shape

train_df = df.drop("ID", axis = 1)
train_df.head()

train_label = df.drop("text", axis = 1)
train_label.head()

"""# Data Cleaning

Replace all digits and punctuation
"""

df.replace('\d+', '', regex=True)
df['text'] = df['text'].str.replace('[^\w\s]', '')
df.head()

"""Make all the text lowercase"""

df['text'] = df['text'].apply(lambda x: x.lower())

"""# Remove Stop Words"""

nltk.download('stopwords')
stop = stopwords.words('english')
df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
df.head()

"""# **Tokenization**"""

import re
def tokenize(text):
    split = re.split("\W+",text) 
    return split
df['text_token']= df['text'].apply(lambda x: tokenize(x.lower()))
df.head()

"""# New Dataframe from the cleaning"""

train_df.to_csv("cleaned_train_df.csv")
train_label.to_csv("cleaned_train_label.csv")

train = "/content/cleaned_train_df.csv"
train = pd.read_csv(train)

test = "/content/cleaned_train_label.csv"
test = pd.read_csv(test)

train.head()

"""Train/Test split and converting each row to strings"""

x = train_df[:1000]
y = train_label[:1000]

data = x['text'].apply(lambda x: np.str_(x))
target = y['ID'].apply(lambda x: np.str_(x))
X_train, X_test, Y_train, Y_test = train_test_split(data, target, test_size=0.3, random_state= 0)
X_train.shape

"""# **Vectorization**"""

tfidf_v = TfidfVectorizer()
tfidf_X_train = tfidf_v.fit_transform(X_train)
tfidf_X_test = tfidf_v.transform(X_test)
tfidf_X_train.shape

tfidf_X_train.toarray()

"""# Classification Model

Logistic Regression
"""

logreg = LogisticRegression(class_weight = 'balanced')
logreg.fit(tfidf_X_train, Y_train)
Accuracy = logreg.score(tfidf_X_test, Y_test)
print(f'Accuracy: {round(Accuracy*100,2)}%')

"""Naive Bayes"""

NB = MultinomialNB()
NB.fit(tfidf_X_train, Y_train)
Accuracy2 = NB.score(tfidf_X_test, Y_test)
print(f'Accuracy: {round(Accuracy2*100,2)}%')

"""Decision Tree"""

clf = DecisionTreeClassifier()
clf.fit(tfidf_X_train, Y_train)
Accuracy3 = clf.score(tfidf_X_test, Y_test)
print(f'Accuracy: {round(Accuracy3*100,2)}%')

"""Passive Aggressive Classifier"""

pac=PassiveAggressiveClassifier(max_iter=50)
pac.fit(tfidf_X_train,Y_train)
y_pred=pac.predict(tfidf_X_test)
score=accuracy_score(Y_test,y_pred)
print(f'Accuracy: {round(score*100,2)}%')

"""Decision Tree Confusion Matrix"""

clf = DecisionTreeClassifier()
clf.fit(tfidf_X_train, Y_train)
y_pred=clf.predict(tfidf_X_test)

print(classification_report(Y_test, y_pred))
print('\n')
print(confusion_matrix(Y_test, y_pred))

"""# **Save The Model**"""

pickle.dump(clf, open('./fake_news_model.pkl', 'wb'))